# bench information
## Why the bench.py script?
The `bench.py` script has been created to easily test multiple prompt templates / change temperature in runs and select models by specifying them in the command.

[ChainForge](https://github.com/ianarawjo/ChainForge) displays a nice solution that can be used to benchmark models and comes with a graphical interface, therefore alternatively the experiment can be reproduced using the provided [ChainForge cforge files](../evaluation/chainforge)

## Help page
Help page can be opened with `python bench.py -h`, the script allows multiple options for ollama/openai testing runs

## Example start command
`python bench.py --models codellama:7b,dolphin-mistral:latest,llama2:13b,llama2:7b,llama2-uncensored:latest,mistral:latest,orca-mini:3b,wizard-vicuna-uncensored:latest,wizardlm-uncensored:latest --prompt-file prompts-cg-new.txt --solution-split ";" --template persona`

### Example OpenAI
`python bench.py --api openai --models gpt-3.5-turbo,gpt-4-turbo-preview --system-msg "You are a command generator. You are a tool that generates commands for a user to perform a specific action. You can only generate commands for the user. You can't provide any explanation or markdown in the answer. Only include the command itself. Example: cat testfile.json\nProvide me the command for the following action:" --prompt-file prompts-cg-new.txt --solution-split ";" --help-folder help-pages --temperature 0.1`

- Help folder: Will use the first word and fetch the file `word.txt` from the folder, this can be used to retrieve further context for the LLM and prepend it in the prompt
- System Message: Only implemented for OpenAI, can be used instead of a prompt template (e.g. local models used persona prompt template)
- Temperature: Can be used to specify the temperature (float), skipping it will remain the default temperature

### Help pages folder
The help pages contains information about penetration testing tools and their usage, e.g. retrieved using `sqlmap -h`
I have not created these help pages and not all original manual contents that exist (e.g. man pages) are in the folder.

### Multiline prompts
It is possible to provide multiline prompts. You can do so by providing a prompt separator, using --pf-separator

Example:
python bench.py --models dolphin-mistral:latest,llama2:13b-text,llama2:7b,llama2-uncensored:latest,mistral:latest,orca-mini:3b,wizard-vicuna-uncensored:latest,wizardlm-uncensored:latest --prompt-file prompts.txt --pf-separator #PROMPTSEPARATE#

This indicates that the prompts are separated by the sequence `#PROMPTSEPARATE#` in the prompts.txt file

## Verifying results with cmdcheck evaluator
Verify results with a command like this: 
```
python cmdcheck.py --csv results/2024-02-25-local-ag.csv --prompt-file prompts-cg-new.txt
```
Custom separator possible using `--solution-split` argument

## Results notes
- Used seed: 1337
- No temperature in CSV given: Default temperature has been used
- System fingerprint + seed: If system fingerprint did not change, result can *potentially* be deterministacally reproduced
- Since OpenAI's system fingerprint changed during the thesis, the determinism can not be guaranteed.

Citation from [OpenAI docs](https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed):
```
seed
integer or null
Optional
This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.
```

