# Send a message to all available OLLama models and retrieve response
# This code is single-threaded, as in the requests are sent one after the other
import requests
import argparse
from app.integrations import ollama, llamacpp, oai
from app.io import csv_handler
from datetime import datetime
import os
import json
from app.llm.config import LLMConfig

def get_api(type: str, config: LLMConfig) -> ollama.OllamaApi | llamacpp.LlamaCppApi:
    if type == 'ollama':
        return ollama.OllamaApi(config)
    if type == 'openai':
        return oai.OpenAIAPI(config)
    if type == 'llamacpp':
        raise NotImplementedError("llamacpp not implemented")
    else:
        raise KeyError(f"Unknown API type: {type}")
    
def load_models(api: ollama.OllamaApi | llamacpp.LlamaCppApi | oai.OpenAIAPI) -> list:
    return [llm['name'] for llm in api.list()['models']]

def insert_template(name: str, prompt: str) -> str:
    templates = json.load(open("templates.json"))
    return templates[name].replace("<PROMPT>", prompt)


def main():
    current_date = datetime.now().strftime("%Y-%m-%d")
    parser = argparse.ArgumentParser()
    parser.add_argument("--message", type=str, required=False, help="Prompt to send to the model")
    parser.add_argument("--api", type=str, required=False, default='ollama', help="API to use, Options: ollama, openai")
    parser.add_argument("--prompt-file", type=str, required=False, help="File containing prompts, one per line")
    parser.add_argument("--pf-separator", type=str, required=False, help="Prompt file sepator, default: newline")
    parser.add_argument("--models", type=str, required=False, help="Comma-separated list of models to use")
    parser.add_argument("--template", type=str, required=False, help="Prompt template to use")
    parser.add_argument("--temperature", type=float, required=False, help="Temperature to use")
    parser.add_argument("--max-tokens", type=int, required=False, help="Max tokens to use")
    parser.add_argument("--system-msg", type=str, required=False, help="System message to use (OpenAI)")
    parser.add_argument("--solution-split", type=str, required=False, help="Separator for the prompt solution")
    parser.add_argument("--help-folder", type=str, required=False, help="Folder that contains help files for commands, e.g. sqlmap.txt, nmap.txt, etc.")
    args = parser.parse_args()
    
    config = LLMConfig(args.api, args.temperature, args.max_tokens, system_message=args.system_msg)
    api = get_api(args.api, config)
    
    
    if not args.message and not args.prompt_file:
        raise ValueError("Either --message or --prompt-file must be provided")
    
    prompts = [args.message] if args.message else []
    models = load_models(api) if not args.models else args.models.split(",")
    
    if args.prompt_file:
        with open(args.prompt_file, "r") as f:
            # Set prompts to be the lines of the file, with no newline characters
            if not args.pf_separator:
                prompts = [line.strip() for line in f.readlines()]
            else:
                prompts = [line for line in f.read().split(args.pf_separator)]
            if args.solution_split:
                    prompts = [prompt.split(args.solution_split)[0] for prompt in prompts]
                  
    originalprompts = prompts.copy()
    if args.template:
        prompts = [insert_template(args.template, prompt) for prompt in prompts]
    if args.help_folder:
        prompts = [f"{open(os.path.join(args.help_folder, str(originalprompts[i].split()[0]).lower().replace(':', '') + '.txt')).read()}\n{prompt}" for i, prompt in enumerate(prompts)]
    
    
    # Swapping those two for loops significantly decreases the performance, because models keep getting
    # reloaded into the GPU
    for llm in models:
        for prompt in prompts:
            response = api.chat(prompt, llm)
            api.print_response(response)
            parsed = api.parse(prompt, response)
            csv_handler.write_response_to_csv(parsed, os.path.join("results", f"{current_date}.csv"))
        
if __name__ == "__main__":
    main()