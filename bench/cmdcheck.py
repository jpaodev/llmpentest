import re
from openai import OpenAI
import os
from datetime import datetime

def get_response_cmd(response):
    pattern = r"```(?:[a-zA-Z]+)?\n([\s\S]*?)\n```"
    matches = re.findall(pattern, response)
    if matches and len(matches) > 0:
        responsecmd = matches[0]
    else:
        responsecmd = response
    pattern = r"`([^`]+)`"
    matches = re.findall(pattern, response)
    if matches and len(matches) > 0:
        responsecmd = matches[0]
    else:
        responsecmd = response
    if "`" in responsecmd:
        responsecmd = responsecmd.replace("`", "")
    if len(responsecmd) < 2:
        print("e")
        print(responsecmd)
    return responsecmd

def check_command_match_openai(command, response, model: str = 'gpt-3.5-turbo'):
        responsecmd = str(get_response_cmd(response)).strip()
        api_key = os.getenv("OPENAI_API_KEY")
        client = OpenAI(api_key=api_key)
        print("Sending OpenAI request", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        print(f"Comparing:\n{command}\n{responsecmd}\n")
        chat_completion = client.chat.completions.create(
        max_tokens=30,
        seed=1337,
    
        messages=[
            {
                "role": "system",
                "content": "Determine whether the two commands achieve the same task or purpose. http instead of https or vice versa is okay. Respond with 'yes' if they do, and 'no' if they don't."
            },
            {
                "role": "user",
                "content": f"CMD 1: {command}\nCMD 2:{responsecmd}",
            }
        ],
        model=model,
       )
        response_content = chat_completion.choices[0].message.content
        print(response_content)
        if "yes" in response_content.strip().lower():
            return True
        return False
    
def check_command_options_match(command, response, extra_args_allowed=False):
    responsecmd = str(get_response_cmd(response)).strip()
        
    # Split the command and response into components
    command_parts = command.split()
    response_parts = responsecmd.split()
    
    # Create sets for the command and response arguments/options, excluding the executable
    command_args = set(command_parts[1:])
    response_args = set(response_parts[1:])  # Exclude executable for direct comparison
    
    # print(f"Comparing:\n{command}\n{responsecmd}\n")
    try:
        # Ensure the command executable is present in the response
        if command_parts[0] != response_parts[0]:
            return False
        
        # Check if all command arguments/options are present in the response
        is_subset = command_args.issubset(response_args)
        
        if extra_args_allowed:
            return is_subset
        else:
            # Ensure no extra arguments/options are present in the response when not allowed
            return is_subset and command_args == response_args
    except Exception as e:
        print(f"Error with {response}")
        return False


# Test cases
test_commands = {
    # Extra args not allowed; should return False because "-x" is an extra argument not present in the command
    "nmap 192.168.178.84 -p- -T4": ("nmap -p- -x -T4 192.168.178.84", False),  
    # Extra args allowed; should return True even if "-x" is extra
    "nmap 192.168.178.84 -p- -T4": ("nmap -p- -x -T4 192.168.178.84 ``` this is some more text ", True),  
    
    # No extra args and all command args present; should return True
    "nmap 192.168.178.84 -x -p- -T4": ("```bashnmap -x -p- -T4 192.168.178.84```", False),  
}

if __name__ == "__main__":
    import pandas as pd
    import argparse
    parser = argparse.ArgumentParser(description='Check command generation results')
    parser.add_argument('--csv', type=str, help='Path to the CSV file')
    parser.add_argument('--prompt-file', type=str, help='Path to the prompt file, instruction and solution separated by a character (default semicolon)')
    parser.add_argument('--openai', type=bool, help='Use OpenAI to check the commands', required=False, default=False)
    parser.add_argument('--allow-extra', type=bool, help='Allow extra arguments in the response', required=False, default=False)
    parser.add_argument("--solution-split", type=str, required=False, default=";", help="Separator for the prompt solution")
    args = parser.parse_args()
    
    df = pd.read_csv(args.csv)

    unique_models = df['model'].unique()
    
    # Filter the DataFrame based on the 'model' column
    
    prompts = []
    
    if args.prompt_file:
        with open(args.prompt_file, "r") as f:
            # Set prompts to be the lines of the file, with no newline characters
            prompts = [line.strip() for line in f.readlines()]
    checked_commands = 0
    for model in unique_models:
        correct_commands = 0
        filtered_df = df.loc[df['model'] == model]
        for prompt in prompts:
            for index, row in filtered_df.iterrows():
                msg, solution = prompt.split(";")
                if msg in row['message']:
                    result = check_command_options_match(solution, row['answer'], extra_args_allowed=args.allow_extra)
                    if args.openai:
                        result = check_command_match_openai(solution, row['answer'])
                        
                    if result:
                        correct_commands += 1
                        # print("Correct: ", solution, row['answer'])
                    checked_commands += 1
        print(f"Model: {model} | Correct commands: {correct_commands}")
    print(f"Overall checked: {checked_commands}")