import os
class LLMConfig:
    def __init__(self, api: str, temperature: float, max_tokens: int, system_message: str = "You are a helpful assistant", more_options: dict = {}):
        self.api = api
        self.OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        self.system_message = system_message
        
        if self.api == "openai":
            self.temperature = 1
            # OpenAI Docs
            # temperature
            # number or null
            # Optional
            # Defaults to 1
            # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
            # We generally recommend altering this or top_p but not both.
        else:
            # Default value for llama.cpp and ollama
            self.temperature = 0.8
            
        self.max_tokens = 400
        self.more_options = more_options
        if temperature or (isinstance(temperature, float) and temperature == 0.0):
            self.temperature = temperature
        if max_tokens:
            self.max_tokens = max_tokens
